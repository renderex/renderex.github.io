[ { "title": "Introduction to RendeRex services in compute and GPU clusters", "url": "/posts/Introduction-to-RendeRex/", "categories": "AI, servers", "tags": "renderex computers, servers, gpu server, ai, tutorial, ai tutorial, virtualization, virtual machine, pcie passthrough, gpu vm, gpu virtual machine", "date": "2022-07-14 00:00:00 +0400", "snippet": "How RendeRex adds value to server salesWhat do we do?RendeRex takes the initial and ongoing DevOps &amp;amp; MLOps required to set up GPU and compute clusters out of the equation for end-users. This means that the data-science team can focus directly on their work, rather than spend countless hours wasted on Linux system administration and MLOps/DevOps.Most large organizations today, especially in the ME, will hire a full-time DevOps/MLOps engineer or team of engineers depending on the scale of the project in order to set up and maintain the cluster.This adds a big layer of inefficiency due to the lack of MLOps/DevOps engineers in the region, as well as the fact that other than the initial setup and deployment, these expensive personnel are rarely required.RendeRex does this automatically for the end-user with the purchase of a cluster, compute or storage node. So what exactly is MLOps/DevOps?MLOps / DevOpsEssentially, there are several options depending on whether the end-user has purchased a single compute node or cluster.Single Node: Baremetal Linux OSS Virtualization environment (Proxmox / XCP-NG) + VFIO Kubernetes cluster built on top of virtualization + VFIOCluster: Vanilla Kubernetes with Docker + VFIO Kubernetes with KubeFlow + VFIO OpenStack with Kubernetes + KubeFlow + VFIOWhat is VFIO?VFIO essentially means that the GPU is passed through entirely as a PCIe device to the virtual machine. The value of this is that the end-user does not have to pay for / maintain an NVIDIA licensing server to utilize vGPU, instead they can simply pass through the entire GPU to the virtual machine / pod / container at no additional cost. This also means that consumer NVIDIA GPU’s can be utilized in GPU-Comptue clusters.What is an OSS Virtualization Environment, and why is it useful?An OSS virtualization environment is essentially the same thing as VMWare, built on a free and open-source platform using Linux QEMU/KVM. We can split a server into multiple virtual machines and pass-through a GPU, or multiple GPU’s to each of them. This is especially useful for small Data Science teams who are working off of a server node.In essence, the server is split into several virtual workstations which can be accessed online, with permanent storage and dynamic resources (that are controlled manually by a system admin). From the data-scientists perspective, it is as if they are working on baremetal Linux.What is baremetal Linux?A baremetal Linux solution is (usually) an Ubuntu 18.04LTS or Ubuntu 20.04LTS Linux machine that is optimized, out-of-the-box, for AI/ML.This entails: Installing NVIDIA drivers Blacklisting the nouveau driver Further kernel optimizations Installation of Docker, CUDA, CuDNN, TFlow, etc. (specific AI/ML packages/frameworks upon request from the end-user)What is a Kubernetes cluster?Kubernetes is an open-source orchestration platform for containerization developed primarily by Google. It was built to solve the problem of scalability of services, and has been widely adopted by the AI/ML community for its own use.Traditionally, data scientists will run their applications as some sort of container (usually Docker), which will then run on top of a single compute node or inside a virtual machine. This works fine when there are 1-2 compute nodes, and we have a small team. However, problems arise when we start adding multiple compute nodes and adding team members to a project.Simply put, Kubernetes allows end users to run their containerized applications on scalable pods, without caring about what node they’re deployed on or which compute node they’re pulling their resources from.The fact that these pods are scalable means that the application can pull resources from multiple compute nodes, creating a cluster of pods, which combine to form a service. This service is then granted a single IP address (even if it is running on several compute nodes) by a load balancer.From the end-user perspective, they could effectively log-into a management or head-node, deploy an application and utilize the entire resources of the cluster; hundreds or even thousands of GPU’s at once.Kubernetes + KubeFlowKubeFlow is a set of applications that run on top of Kubernetes and simplify the ML/AI development and production process for data scientists. It also makes the process portable to other platforms (AWS, Azure, GKE, etc.), as well as scalable.KubeFlow includes: JupyterHub for creating Jupyter Notebooks (visualized, on-the-go, in-browser python programming) TensorFlow Serving for model serving TensorFlow Training for model training Katib for hyperparameter search and tuning Model version control KubeFlow Pipelines, an easy-to-use comprehensive UI for running experiments, etc. MinIO server for storing artifacts &amp;amp; more…What is OpenStackOpenStack takes things one step further when utilizing a large amount of resources. This allows you to turn your datacenter hardware into an on-prem private cloud, and have your team use it exactly the same way as they would use Amazon AWS, Google GKE, etc.From the end-user / data-science team’s perspective, the UI will feel identical to familiar platforms (AWS, GKE, etc.).It also has the ability to integrate with public cloud providers so that you can essentially build a hybrid-cloud, utilizing both on-prem resources and, for example, Google GKE compute resources.When combining OpenStack with Kubernetes running on top of it, running KubeFlow with VFIO enabled, essentially the end-user now has a full, enterprise-grade production environment which can be deployed with either consumer or enterprise-grade GPU’s.The main selling-point, as far as the end-user is concerned, is the pay and forget aspect, as there are no licensing fees involved in the entire solution. Everything is free, open-source, enterprise-grade and utilized in production by industry giants in the AI/ML space.High-AvailabilityWhen utilizing multiple nodes in a professional/enterprise environment, it is important that a single node’s failure does not take down the entire cluster. RendeRex will usually deploy a cluster with 3 head/master nodes and n compute nodes. This means that if a head/master node fails, or any of the compute nodes, the remainder of the cluster will continue functioning as normal. By definition, this is known as a High Availability (HA) Cluster.For the compute nodes, Kubernetes itself is self-healing, meaning that if a compute node dies, it will simply migrate the compute pod to a different compute node and continue serving the service.The principles of high-availability also applies to storage nodes, as well as databases. For example, for Kubernetes to function it utilizes etcd databases, which are also deployed in pairs of three, so that if one fails, the information about cluster health, status, etc. is not lost, and the service continues as normal while maintenance is performed.StorageRendeRex will set up storage nodes for its clients and attach them to the compute cluster with either free, open-source (FOSS) software, or proprietary software. Our engineers are familiar with Lustre, WekaFS, TrueNAS Core, TrueNAS Scale, and more.An example set-up using TrueNAS Scale would be: RAIDZ1 ZFS SSD Pool Attached to Kubernetes as an NFS share RAIDZ2 ZFS HDD Archive pool Automatic, periodic backups of SSD data Local compute node drives set as an NFS cache using cachefilesd in LinuxFor single-node setups, TrueNAS can also easily spin up an S3 MinIO server for object-storage.Additional DevOps / MLOps support packagesEverything we’ve mentioned so far comes included at no additional cost to the end-user and is included with the purchase of a compute node or compute cluster from RendeRex.In addition, RendeRex also offers support packages, billed annually, for either phone/e-mail support or an on-prem engineer from RendeRex to be stationed at the end-users’ datacenter.ConclusionWe hope you’ve enjoyed the brief summary of some of what RendeRex has to offer its clients in terms of added-value to server sales, specifically in the AI/ML or compute space. For specific enquiries, please contact info@renderex.ae.Alternatively, for general information about RendeRex and it’s services, please visit www.renderex.ae." }, { "title": "Enabling Hugepages in Proxmox", "url": "/posts/Enabling-hugepages/", "categories": "AI, tutorial, servers, proxmox", "tags": "renderex computers, servers, workstations, proxmox, virtualization, ai, tutorial, ai tutorial, linux", "date": "2022-06-14 00:00:00 +0400", "snippet": "What are Hugepages? Memory is managed in blocks known as pages. On most systems, a page is 4Ki. 1Mi of memory is equal to 256 pages; 1Gi of memory is 256,000 pages, and so on. CPUs have a built-in memory management unit that manages a list of these pages in hardware. The Translation Lookaside Buffer (TLB) is a small hardware cache of virtual-to-physical page mappings. If the virtual address passed in a hardware instruction can be found in the TLB, the mapping can be determined quickly. If not, a TLB miss occurs, and the system falls back to slower, software-based address translation, resulting in performance issues. Since the size of the TLB is fixed, the only way to reduce the chance of a TLB miss is to increase the page size. A huge page is a memory page that is larger than 4Ki. On x86_64 architectures, there are two common huge page sizes: 2Mi and 1Gi. Sizes vary on other architectures. To use huge pages, code must be written so that applications are aware of them. Transparent Huge Pages (THP) attempt to automate the management of huge pages without application knowledge, but they have limitations. In particular, they are limited to 2Mi page sizes. THP can lead to performance degradation on nodes with high memory utilization or fragmentation due to defragmenting efforts of THP, which can lock memory pages. For this reason, some applications may be designed to (or recommend) usage of pre-allocated huge pages instead of THP.Why and when to use HugepagesAnytime you want to create a VM with more than 64GB of RAM, you’ll want to use Hugepages.Enable HugepagesTo enable Hugepages in Proxmox, in the “hardware tab”, under “CPU”, you want to enable the hugepages flag, which is titled pdpe1gb.Next, SSH into Proxmox itself and enable hugepages in GRUB:GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet intel_iommu=on hugepagesz=1G default_hugepagesz=2M&quot;That’s it, you’re done! For pre-built, pre-configured GPU servers for AI, be sure to visit:www.renderex.ae" }, { "title": "Creating a new VM in Proxmox", "url": "/posts/Creating-a-new-VM/", "categories": "AI, tutorial, servers, proxmox", "tags": "renderex computers, servers, gpu server, ai, tutorial, ai tutorial, virtualization, virtual machine, pcie passthrough, gpu vm, gpu virtual machine, training server", "date": "2022-06-14 00:00:00 +0400", "snippet": "1. Start the creation wizardIn the top-right, start the creation wizard by opening up a “New VM”. Give your VM a name, and change the VM ID to whatever you want, or let Proxmox choose an ID for you.2. Add the ISO imageNavigate to the OS tab and select the ISO image you’d like to create the VM with. Proxmox should automatically choose the Kernel version and the OS type for you, but double-check just to make sure.3. System TabNext, move to the ‘System’ tab, you can either leave everything as the default here, or you can switch to OVMF for UEFI support. For this tutorial we’ll leave everything as default.4. Setting up storage disksMove to the disks tab and make sure to select “VirtIO” block for best performance. Set the disk size in GiB, you can leave the rest of the settings to their defaults.5. CPU setupChoose how many CPU cores you’d like to assign the VM, and for ‘type’, we recommend setting the CPU to ‘host’, to allow use of the AVX-512 instruction set. This is necessary for some applications in TensorFlow.6. Memory (RAM)In the memory tab, you can select how much RAM you’d like to assign to the VM, note: this must be set in Megabytes. In this example, we are assigning 32GB of memory to the VM.A ballooning device means that the VM can be assigned an amount of RAM, but will not utilize the full amount until necessary. This allows for RAM to be shared across VM’s - think of this as a maximum allocated RAM option. For values larger than 64GB of RAM, we recommend you turn on “hugepages”. We will add documentation on how to do this elsewhere.7. NetworkFor the best performance, set the network model to “VirtIO (paravirtualized)”. In this tab you can also assign the VM to a VLAN if you decide.8. Review and confirmThe final tab is a summary of your inputs, review the information and click “Finish”. You can also check the box “Start after created” if you’d like.9. Start your VMIf you haven’t already, find your VM in the left pane, right click and “Start” the VM.10. Install the OSIn the “Console” tab, proceed to install the VM as normal.11. Adding/Removing HardwareThe “Hardware” tab allows you to control the hardware attached to your VM. This includes CD/DVD-ROM drives, PCIe devices, etc. From here you can “detach” the CD/DVD drive after the installation is complete.12. Final SetupGo ahead and use the console to set-up your networking inside the VM, and update. Optionally, you can also setup a guest agent in your VM so that you can control and manage things like the IP address from the host itself (Proxmox).On Linux this is very easy to achieve. On Ubuntu:sudo apt-get install qemu-guest-agentIf it doesn’t start automatically:systemctl start qemu-guest-agentThat’s it! Your new VM is set up and ready to go. For pre-built, pre-configured GPU servers for AI, be sure to visit:https://renderex.ae/servers/ml-server" }, { "title": "CPU Pinning in Proxmox", "url": "/posts/CPU-Pinning/", "categories": "AI, tutorial, servers, proxmox", "tags": "renderex computers, servers, workstations, proxmox, virtualization, ai, tutorial, ai tutorial, linux", "date": "2022-06-14 00:00:00 +0400", "snippet": "What is CPU pinning?Generally in virtualization, the host will spread the workload of a VM accross all of the available host cores. However, with CPU pinning, you can ‘pin’ the tasks of a VM to specific cores on the host, allowing it to fully utilize those cores and ‘blocking’ them from performing any other tasks.This can greatly increase CPU performance on a VM and provide near-native CPU performance on thoe cores.Pinning the coresSSH into the host (Proxmox) and run the command:taskset --cpu-list --all-tasks --pid 0-7 $(&amp;lt; /run/qemu-server/103.pid)In this case, replace 0-9 with the number of CPU cores you want to pin to this virtual machine. note: CPU core count starts at 0, so in this example we are pinning 8 cores to the VM with ID 103.In addition, replace 103.pid with the ID number of the VM you’d like to pin.Checking if the cores are pinnedYou can easily check if the CPU pinning has been successful by running htop on the host machine and stressing the VM (e.g. stress --cpu [number of cores])Automating the processWe can automate this process in Proxmox by creating a ‘hookscript’. In this example, we create a hook script for VM 103, to permanently pin 8 CPU cores to it.nano /etc/pve/qemu-server/103.confto the bottom of the file, add:hookscript: local:snippets/cpu-pinning-103.shNavigate to the local data storecd /var/lib/vzIf there isn’t a snippets directory, go ahead and create one, then create the hookscript inside it:cd snippets &amp;amp;&amp;amp; nano cpu-pinning-103.sh#!/bin/bashvmid=&quot;$1&quot;phase=&quot;$2&quot;cpuset=&quot;0-7&quot;if [[ &quot;$phase&quot; == &quot;post-start&quot; ]]; then main_pid=&quot;$(&amp;lt; /run/qemu-server/$vmid.pid)&quot; taskset --cpu-list --all-tasks --pid &quot;$cpuset&quot; &quot;$main_pid&quot;fiExit out of nano and make the script executable: chmod +x cpu-pinning-103.shYou can also ‘unhook’ this script from the VM at anytime from the web GUI, as there should now be a “Hookscript” option at the bottom of the “Options” tabin your VM’s pane.That’s it, you’re done! For pre-built, pre-configured GPU servers for AI, be sure to visit:www.renderex.ae" }, { "title": "Adding a new ISO to Proxmox", "url": "/posts/Adding-an-ISO/", "categories": "AI, tutorial, servers, proxmox", "tags": "renderex computers, servers, gpu server, ai, tutorial, ai tutorial, virtualization, virtual machine, pcie passthrough, gpu vm, gpu virtual machine", "date": "2022-06-14 00:00:00 +0400", "snippet": "1. Navigate to your storageNavigate to the storage dedicated to your ISO images and click the “ISO Images” tab on the left hand side.2. Download the ISOAbove the ISO images, you have two options, either to upload your own or to download from URL. For the sake of this example, we’ll be downloading Ubuntu 20.04LTS directly from Canonical.3. Get the download URLNavigate to the website of the ISO you’d like to download and copy the download link.4. Paste the download link back into the Proxmox GUI. Note that once the ISO is downloaded into your Proxmox server, it will stay there, meaning you don’t need to download the ISO every time you want to use it for a VM, it will be available for all future VM’s until you delete it.5. (Optional) ChecksumIf you’re downloading an obscure ISO and you’re unsure of the source, you can always open the advanced tab and Proxmox will compare the checksum of the downloaded ISO for you." }, { "title": "Setting up a Cloud Image template and Cloud Init", "url": "/posts/Setting-up-a-cloud-image-VM/", "categories": "AI, servers, tutorial", "tags": "renderex computers, servers, gpu server, ai, tutorial, ai tutorial, virtualization, virtual machine, pcie passthrough, gpu vm, gpu virtual machine, linux, proxmox", "date": "2022-05-31 00:00:00 +0400", "snippet": "What is a ‘Cloud Image’?Cloud Images are lightweight, faster, certified cloud-ready versions of operating systems. They have Cloud Init pre-installed, meaning they can be configured using ‘Cloud Config’.This works great combination with setting up a GPU server in Proxmox.Getting StartedFirst, you’ll want to decide which version of Ubuntu you’d like. We generally recommend 18.04LTS or 20.04LTS for AI applications.Once you’ve decided, copy one of the URL’s from here.Next, login to your Proxmox server and download it, being sure to use the URL of the version of your choosing:wget https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.imgCreate a VMNext, let’s create a virtual machineqm create 8000 --memory 4096 --core 4 --name ubuntu-cloud --net0 virtio,bridge=vmbr0Import the new disk to local-lvm storage (or whichever disk you prefer)qm importdisk 8000 focal-server-cloudimg-amd64.img local-lvmAttach the new disk to the virtual machine as a scsi drive on the scsi controllerqm set 8000 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-8000-disk-0Add a ‘cloud init’ drive:qm set 8000 --ide2 local-lvm:cloudinitMake the cloud init drive bootable and restrict BIOS to boot from disk only:qm set 8000 --boot c --bootdisk scsi0Add a serial console:qm set 8000 --serial0 socket --vga serial0Setting up hardwareIf you’d like to make any hardware changes at this point, you should do so in the GUI. However, do not start the VM.It’s best to keep the minimum amount of resources possible, as this will serve as a template. So don’t attach any GPU’s yet, and we recommend you keep the storage capacity low, as we can always expand this later.Copy - PasteNow let’s create a template:qm template 6000and clone it:qm clone 6000 135 --name noor1 --full(Replace noor1 with any name you want to give the VM, and 6000 with any ID)For pre-built, pre-configured GPU servers for AI, be sure to visit:https://renderex.ae/servers/ml-server" }, { "title": "Extending an LVM storage pool in Proxmox", "url": "/posts/Extending-LVM-pool/", "categories": "AI, servers, proxmox", "tags": "renderex computers, servers, gpu server, ai, tutorial, ai tutorial, virtualization, virtual machine, pcie passthrough, gpu vm, gpu virtual machine, linux, proxmox", "date": "2022-05-31 00:00:00 +0400", "snippet": "Format the driveFirst, list the available disks:fdisk -lFormat the disk you want to add, creating a partition, in this case we will create sdf1 on the disk sdf:cfdisk /dev/sdfCreate a physical volumepvcreate /dev/sdf1Extend it:vgextend pve /dev/sdf1lvextend /dev/pve/data -L +7.3T(Replace +7.3T with the size of the disk space being added)Done!For pre-built, pre-configured GPU servers for AI, be sure to visit:https://renderex.ae/servers/ml-server" }, { "title": "Setting up a Workstation for AI", "url": "/posts/Setting-up-worktation-AI/", "categories": "AI, tutorial, workstations", "tags": "renderex computers, servers, workstations, ai, tutorial, ai tutorial, linux", "date": "2022-05-30 00:00:00 +0400", "snippet": "Getting StartedThe first thing you’ll want to do on your new Ubuntu installation is to blacklist nouveau. Nouveau is an open-source Linux graphics driver which is loaded at boot, but this has been known to interfere with certain functions of the NVidia driver, and can cause problems, especially when installing ‘beta’ versions of NVidia drivers.First, open up a terminal and type:sudo nano /etc/modprobe.d/blacklist-nouveau.confand insert the following:blacklist nouveaublacklist lbm-nouveauoptions nouveau modeset=0alias nouveau offalias lbm-nouveau offSave the file and exit out of nano (or your preferred text editor).Disable nouveau in the Linux kernel:echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/blacklist-nouveau.confUpdate initramfssudo update-initramfs -uInstall the NVidia driverssudo apt install nvidia-driver-510This will install NVidia driver version 510.Once the installation is complete, don’t forget to reboot.sudo rebootChecking that the driver is correctly intalledTo ensure that the driver has loaded properly, open up a terminal and enter:nvidia-smiIf successfull, you should see your GPU(s) listed in the terminal. It should look something like this:+-----------------------------------------------------------------------------+| NVIDIA-SMI 497.09 Driver Version: 497.09 CUDA Version: 11.5 ||-------------------------------+----------------------+----------------------+| GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA Tesla T4 WDDM | 00000000:01:00.0 On | N/A || 0% 39C P8 14W / 75W | 1228MiB / 8192MiB | 5% Default || | | N/A |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=============================================================================|Tip: If you’d like to continuously watch your GPU’s, you can use:watch nvidia-smithis will give you an overview of the sensor output on your GPU(s) at an interval of every 2 secondsThat’s it! you’re now ready to install CUDA, etc., and begin building your training environment.Optional: Set CPU governor to ‘performance mode’If you’d like to set the CPU to ‘performance mode’, open up a terminal:sudo apt-get install cpufrequtilsecho &#39;GOVERNOR=&quot;performance&quot;&#39; | sudo tee /etc/default/cpufrequtilssudo systemctl disable ondemandFor pre-built, pre-configured AI workstations, be sure to visit:https://renderex.ae/workstations/data-science.html" }, { "title": "Setting up a GPU Server for AI virtual machines with Proxmox", "url": "/posts/Setting-up-a-server-for-AI/", "categories": "AI, tutorial, servers", "tags": "renderex computers, servers, gpu server, ai, tutorial, ai tutorial, virtualization, virtual machine, pcie passthrough, gpu vm, gpu virtual machine", "date": "2022-05-30 00:00:00 +0400", "snippet": "What is Proxmox and the advantage of Virtual MachinesProxmox is a debian-based, open-source virtualization environment which allows you to create virtual machines using QEMU/KVM. It has a user-friendly GUI interface and interacting with it through terminal is a breeze as the backend is just the very familiar Debian.If you have a small team working on a single GPU server, (like the RendeRex 8x GPU Server, the RAPTOR-8), it makes sense to be able to split the GPU’s into several VM’s, allowing each team to seamlessly take advantage of however many GPU’s they’d like, and move them between VM’s without much effort.Here, we’ll go over the very basics to get started.Pre-requisitesThe first thing you’ll want to do is make sure that virtualization is enabled on your hardware. This is a setting which can be found in the BIOS. For Intel, you’ll want to enable VT-d. For AMD, look for IOMMU. Most modern CPU’s support virtualization and its simply a matter of enabling it. If you bought a pre-configured server from RendeRex, this feature is already enabled for you.We’ll be taking advantage of PCIe pass-through, allowing us to completely pass-through a PCIe device (in this case, a GPU) from the host (Proxmox), to the guest (an Ubuntu virtual machine).This will allow us to essentially transform our GPU-server into fully functional, dynamic, independent workstations which can be used by members of the AI team.Installing ProxmoxThe first thing you’ll want to do is grab the latest version of Proxmox from here.Once its installed, you can access the Web GUI at: https://[proxmox-server-ip]:8006Configuring Proxmox &amp;amp; setting up IOMMU1. GrubGo ahead and SSH into your server, let’s start by making some changes to grub:(assuming you’re logged in as root:)nano /etc/default/grubReplace this lineGRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet&quot;with:GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet intel_iommu=on&quot;or, for AMD CPU’sGRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet amd_iommu=on&quot;Next, update grub to apply the changes using:update-grub2. VFIO modulesTo load the VFIO modules needed for PCIe passthrough:nano /etc/modulesand add:vfiovfio_iommu_type1vfio_pcivfio_virqfdSave and exit out of nano. Next, you’ll need to run the two following commands:echo &quot;options vfio_iommu_type1 allow_unsafe_interrupts=1&quot; &amp;gt; /etc/modprobe.d/iommu_unsafe_interrupts.confecho &quot;options kvm ignore_msrs=1&quot; &amp;gt; /etc/modprobe.d/kvm.conf3. Block host (Proxmox) from loading driversWe’ll need to make sure that the host (in this case Proxmox) cannot load the GPU drivers. We can achieve this easily through:nano /etc/modprobe.d/pve-blacklist.confand insert:blacklist nvidiafbblacklist nvidiablacklist radeonblacklist nouveauSave and exit nano, then reboot.3. Optional (check if IOMMU groupings are correct)You can check if the IOMMU groupings are correct by running the following script:#!/bin/bashfor d in /sys/kernel/iommu_groups/*/devices/*; do n=${d#*/iommu_groups/*}; n=${n%%/*} printf &#39;IOMMU Group %s &#39; &quot;$n&quot; lspci -nns &quot;${d##*/}&quot;doneIf done correctly, the output should look something like this:...IOMMU Group 51: 1d:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA102 [GeForce RTX 3090] [10de:2204] (rev a1) 1d:00.1 Audio device [0403]: NVIDIA Corporation GA102 High Definition Audio Controller [10de:1aef] (rev a1)IOMMU Group 52: 1e:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA102 [GeForce RTX 3090] [10de:2204] (rev a1) 1e:00.1 Audio device [0403]: NVIDIA Corporation GA102 High Definition Audio Controller [10de:1aef] (rev a1)IOMMU Group 53: 1f:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA102 [GeForce RTX 3090] [10de:2204] (rev a1) 1f:00.1 Audio device [0403]: NVIDIA Corporation GA102 High Definition Audio Controller [10de:1aef] (rev a1)...If you can see that each GPU has its own IOMMU grouping, then everything went right.4. Setting up the VMOnce the server has rebooted, access the web GUI (port 8006).We won’t go over how to set-up an Ubuntu VM here, but you can check out our guide on how to set up a ‘cloud image’ version of Ubuntu.Attaching the GPU:Once you have your Ubuntu VM set-up, let’s go ahead and attach the GPU. Under ‘PCIe devices’, scroll down until you find the IOMMU grouping/vendor ID of the GPU you’d like to attach and select it.CPU:Make sure that you set the CPU to ‘host’ in order to enable the AVX-512 instruction-set (required for TensorFlow).OptionalIf you’d like to ‘pin’ CPU cores, i.e., reserve specific cores for a virtual machine for increased performance of that CPU, this can be achieved with:taskset --cpu-list --all-tasks --pid [cores] $(&amp;lt; /run/qemu-server/[pid-of-VM.pid])For example, pinning the first 16 cores of the CPU to a VM with ID 101 would look like this:taskset --cpu-list --all-tasks --pid 0-15 $(&amp;lt; /run/qemu-server/101.pid)Note that CPU core ID’s begin from 0 instead of 1.Networking:In order to take full advantage of networking capabilities, make sure that VirtIO is selected for the network device.Hardware Summary:Your hardware should look something like this:That’s it! Go ahead and start your Ubuntu VM, install the NVidia drivers, and proceed to set it up like you would a workstation.That wraps up this tutorial. Stay tuned for further optimizations of Proxmox, VM’s for AI, storage for VM’s, Cloud-init Ubuntu Templates, and more!For pre-built, pre-configured GPU servers for AI, be sure to visit:https://renderex.ae/servers/ml-server" } ]
